# -*- coding: utf-8 -*-
"""CKD_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kvTqhzKddat9_vTA02VAfn9PVnRUJFw

IMPORTING LIBRARIES
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel,SelectKBest, chi2
from sklearn.linear_model import LogisticRegression,Lasso
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor
from sklearn.svm import LinearSVC

"""IMPORTING DATASET"""

from google.colab import drive
drive.mount('/content/drive')

# READING THE DATA
ds=pd.read_csv('/content/drive/MyDrive/csv_result-chronic_kidney_disease.csv')

# DISPLAYING FIRST FEW ROWS OF THE DATASET
ds.head()

# DISPLAYING THE SHAPE OF THE DATASET
ds.shape

# DISPLAYING THE INFORMATION ABOUT THE DATASET
ds.info()

"""DATA PREPROCESSING"""

# DROPPING THE ID COLUMN
ds=ds.drop('id',axis=1)

# CONVERTING '?' TO NULL TYPE
ds.replace('?',np.nan,inplace=True)

# CONVERTING NUMERICAL COLUMNS TO NUMERIC DATA TYPE
columns_to_convert=['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','pcv','wbcc','rbcc']
ds[columns_to_convert] = ds[columns_to_convert].apply(pd.to_numeric, errors='coerce')

# IDENTIFYING CATEGORICAL COLUMNS
categorical_columns=ds.select_dtypes(include=object).columns
print(categorical_columns)

# DISPLAYING DIFFERENT VALUES TAKEN BY EACH CATEGORICAL VARIABLES
for each in categorical_columns:
  print(each)
  print(ds[each].value_counts(),'\n')

# ENCODING CATEGORICAL VARIABLES
dict={'normal':1,'abnormal':0,'present':1,'notpresent':0,'good':1,'poor':0,'yes':1,'no':0}
ds[categorical_columns]=ds[categorical_columns].replace(dict)
ds.head()

# ENCODING THE TARGET VARIABLE
ds['class']=ds['class'].map({'ckd':1,'notckd':0})

# COUNT OF MISSING VALUES
ds.isnull().sum()

"""IMPUTING MISSING VALUES"""

# KNN IMPUTATION FOR MISSING VALUES
from sklearn.impute import KNNImputer
knnImp=KNNImputer(n_neighbors=3)
knnImp_ds=pd.DataFrame(knnImp.fit_transform(ds),columns=ds.columns)

# COUNT OF MISSING VALUES IN KNN IMPUTED DATASET
knnImp_ds.isnull().sum()

# ITERATIVE IMPUTER FOR MISSING VALUES
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imputer=IterativeImputer(estimator=RandomForestRegressor(),random_state=0)
itImp_ds=pd.DataFrame(imputer.fit_transform(ds),columns=ds.columns)

# COUNT OF MISSING VALUES IN ITERATIVE IMPUTED DATASET
itImp_ds.isnull().sum()

# MICE FOREST IMPUTATION FOR MISSING VALUES
! pip install miceforest --no-cache-dir

import miceforest as mf
kernel= mf.ImputationKernel(ds,save_all_iterations=True,random_state=10)
kernel.mice(2)
mf_ds= kernel.complete_data()

mf_ds.isnull().sum()

"""DATA VISUALISATION"""

# OCCURENCE OF THE TARGET VARIABLE
plt.figure(figsize=(5,4))
sns.countplot(x='class',data=ds,hue='class',palette='Set1')
plt.title('Target Variable')
plt.xticks([0,1],['ckd','notckd'])
plt.show()

# CORRELATION MATRIX
plt.figure(figsize=(20,20))
data=ds.corr()
sns.heatmap(data=data, annot = True,fmt='.1f',cmap='coolwarm')
plt.show()

ds.corr()[['class']].style.background_gradient(cmap='BuGn')

"""FEATURE SCALING

"""

# FUNCTION FOR NORMALISATION OF VARIABLES
def feature_scaling(x):
  xcol=x.columns
  sc=StandardScaler()
  x=pd.DataFrame(sc.fit_transform(x),columns=xcol)
  return x

"""FEATURE SELECTION"""

# FUNCTION FOR FEATURE SELECTION
def feature_selection(x,y,classifier_name,classifier):
  selector = SelectFromModel(Lasso(alpha=0.1))
  x_selected = pd.DataFrame(selector.fit_transform(x, y), columns=x.columns[selector.get_support(indices=True)])
  return x_selected

"""OPTIMAL TRAIN-TEST SPLIT"""

# FUNCTION TO FIND THE OPTIMAL TRAIN-TEST SPLIT RATIO
def find_optimal_split(x,y,classifier,splitting_ratios):
  optimal_ratio=None
  max_accuracy = 0.0
  for test_ratio in splitting_ratios:
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_ratio,random_state=42)
    accuracy = model_performance(x_train,x_test,y_train,y_test,classifier)
    if accuracy> max_accuracy:
      max_accuracy = accuracy
      optimal_ratio = round(test_ratio,2)
  return optimal_ratio

# FUNCTION TO FIT THE MODEL AND CALCULATE IT'S PERFORMANCE
def model_performance(x_train,x_test,y_train,y_test,classifier):
  classifier.fit(x_train,y_train)
  y_pred=classifier.predict(x_test)

  accuracy=accuracy_score(y_test,y_pred)

  return accuracy

# DATA PROCESSING AND MODEL BUILDING FOR EACH DATASET
def model_building(datasets,classifiers,splitting_ratios):
  row1=[]
  acc=[]

  for df_name,df in datasets.items():
    row=[]
    acc_row=[]
    print("\nDataframe:",df_name,"\n")

# SPLITTING OF INDEPENDENT AND DEPENDENT COLUMNS
    x=df.drop('class',axis=1)
    y=df['class']

# FEATURE SCALING
    x=feature_scaling(x)

# FEATURE SELECTION AND OPTIMAL TEST RATIO SPLIT
    for classifier_name, classifier in classifiers.items():
      x_selected=feature_selection(x,y,classifier_name,classifier)
      test_ratio=find_optimal_split(x,y,classifier,splitting_ratios)
      row.append(test_ratio)
      x_train, x_test, y_train, y_test = train_test_split(x_selected, y, test_size=test_ratio, random_state=42)
      accuracy=model_performance(x_train, x_test, y_train, y_test,classifier)
      acc_row.append(accuracy)
    print("Selected features:",x_selected.columns)

    row1.append(row)
    acc.append(acc_row)

  opt_df=pd.DataFrame(row1,index=datasets.keys(),columns=classifiers.keys())
  output=pd.DataFrame(acc,index=datasets.keys(),columns=classifiers.keys())

  return opt_df,output

datasets={"KNN imputed":knnImp_ds,"Iterative imputed":itImp_ds,"Mice forest":mf_ds}
classifiers={"Logistic regression":LogisticRegression(),\
             "Decision Tree":DecisionTreeClassifier(),
             "Random Forest":RandomForestClassifier(),
             "Support vector":LinearSVC()}
splitting_ratios = np.linspace(0.2,0.5,4)

opt_df,output=model_building(datasets,classifiers,splitting_ratios)

"""MODEL BUILDING"""

# DIFFERENT MODEL BUILDING
acc_data=[]

for df_name,df in datasets.items():
    accuracy_row=[]

# SPLITTING OF INDEPENDENT AND DEPENDENT COLUMNS
    x=df.drop('class',axis=1)
    y=df['class']

# FEATURE SELECTION AND MODEL BUILDING
    for classifier_name, classifier in classifiers.items():
      x_selected=feature_selection(x,y,classifier_name,classifier)
      x_scaled=feature_scaling(x_selected)
      x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.25, random_state=42)
      accuracy=model_performance(x_train, x_test, y_train, y_test,classifier)
      accuracy_row.append(accuracy)

    acc_data.append(accuracy_row)

accuracy_table=pd.DataFrame(acc_data,index=datasets.keys(),columns=classifiers.keys())

"""RESULTS"""

# TABLE OF OPTIMAL TRAIN-TEST SPLIT RATIO
print("Optimal train-test split ratio for each dataset and models:")
opt_df

# ACCURACY SCORE FOR EACH MODEL AND DATASET UNDER OPTIMAL TRAIN-TEST SPLIT
print("Accuracy scores of different datasets and the model:")
output

# ACCURACY TABLE OF EACH DATASET AND THE MODELS
print("Accuracy scores of different datasets and the model:")
accuracy_table

plt.figure(figsize=(8,6))
accuracy_table.plot(kind='line',stacked=False,marker='o').tick_params(labelrotation=360)
plt.xlabel("Datasets")
plt.ylabel("Accuracy")
plt.title("Comparison of different models")
plt.show()

